# Environment variables for Context Memory Store
# Copy this file to .env and customize as needed

# Neo4j Configuration
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=contextmemory

# Grafana Configuration  
GRAFANA_PASSWORD=contextmemory

# Project Configuration Overrides
# Note: Ollama should be running externally on the host machine
PROJECT_NAME=context-memory-store
PROJECT_DESCRIPTION="Context Memory Store for AI Agents"

# LLM Configuration
LLM_API_BASE=http://host.docker.internal:11434/v1
LLM_CHAT_MODEL=llama3
LLM_EMBEDDING_MODEL=mxbai-embed-large

# Vector Store Configuration
VECTOR_STORE_HOST=localhost
VECTOR_STORE_PORT=6333
VECTOR_STORE_COLLECTION=context-memory

# Graph Store Configuration
GRAPH_STORE_URI=bolt://localhost:7687
GRAPH_STORE_USERNAME=neo4j
GRAPH_STORE_PASSWORD=contextmemory

# API Configuration
API_HOST=0.0.0.0
API_PORT=8080

# Monitoring Configuration
PROMETHEUS_ENABLED=true
METRICS_INTERVAL=15
LOG_LEVEL=INFO

# Development Configuration
ASPNETCORE_ENVIRONMENT=Development
DOTNET_ENVIRONMENT=Development